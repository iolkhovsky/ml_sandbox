{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8e6a10",
   "metadata": {},
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e21e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 64.6M  100 64.6M    0     0  8463k      0  0:00:07  0:00:07 --:--:-- 11.7M000:03  0:00:13 3950k0:14  0:00:04  0:00:10 4512k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  115M  100  115M    0     0  12.8M      0  0:00:09  0:00:09 --:--:-- 13.7M05  0:00:03 14.3M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  169k  100  169k    0     0   301k      0 --:--:-- --:--:-- --:--:--  181k:-- --:--:-- --:--:--  307k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  298k  100  298k    0     0   488k      0 --:--:-- --:--:-- --:--:--  497k\n",
      "Downloading completed\n"
     ]
    }
   ],
   "source": [
    "!./download_opus_100_ru_en.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8349a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_en.txt  test_ru.txt  train_en.txt train_ru.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae159355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you only stay there.\r\n",
      "I don't know how you do it, Pop, carrying these boxes around every day.\r\n",
      "We might have a slight edge in mediation.\r\n",
      "How long is it going to take you to get him what he needs?\r\n",
      "On 1 April President of the Nagorno Karabagh Republic Bako Sahakyan met head of the General Staff of the Republic of Armenia's Armed forces colonel-general Yuri Khachaturov.\r\n",
      "Mr Priesner also noted that the E-justice management system has not only improved case management, but has also led to a significant streamlining in procedures.\r\n",
      "You don't like chicken noodle soup?\r\n",
      "Posted: 14 May 2005, 20:31\r\n",
      "Now, for a minute, I thought maybe he was being tailed.\r\n",
      "« : 26 Октябрь 2017, 06:50:24 »\r\n"
     ]
    }
   ],
   "source": [
    "!head data/test_en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f8e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Только бы не вылететь.\r\n",
      "И как ты только справляешься, папа, таская эти коробки взад-вперед целый день.\r\n",
      "Возможно, у нас есть небольшое преимущество в переговорах.\r\n",
      "Сколько времени вы будете делать то, что ему нужно?\r\n",
      "1 апреля Президент НКР Бако Саакян принял начальника Генштаба Вооруженных сил Республики Армения генерал-полковника Юрия Хачатурова.\r\n",
      "Г-н Приснер также упомянул, что система электронного правосудия не только позволила улучшить процесс ведения дел, но также способствует значительному упорядочению процедур.\r\n",
      "- Неплохо, да.\r\n",
      "Posted: 15 Dec 2006, 00:07\r\n",
      "И на минутку я подумал, что за ним могут следить.\r\n",
      "«: 11 Октябрь 2011, 17:15:34»\r\n"
     ]
    }
   ],
   "source": [
    "!head data/test_ru.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf1947",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4c28c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from enum import IntEnum\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SpecToken(IntEnum):\n",
    "    START = 0\n",
    "    STOP = 1\n",
    "    UNK = 2\n",
    "    PAD = 3\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preprocessor=None, special_tokens=SpecToken):\n",
    "        self.preprocessor = preprocessor if preprocessor else self._default_preprocessor\n",
    "        self.tokens = special_tokens\n",
    "        self._word2id = None\n",
    "        self._id2work = None\n",
    "\n",
    "    def encode(self, word):\n",
    "        if word in self._word2id:\n",
    "            return self._word2id[word]\n",
    "        else:\n",
    "            return int(self.tokens.UNK)\n",
    "\n",
    "    def decode(self, token):\n",
    "        token = int(token)\n",
    "        if token in self._id2word:\n",
    "            return self._id2word[token]\n",
    "        else:\n",
    "            return str(self.tokens.UNK)\n",
    "\n",
    "    def encode_line(self, line):\n",
    "        return [self.encode(x) for x in self.preprocessor(line).split(\" \")]\n",
    "\n",
    "    def decode_line(self, tokens):\n",
    "        return \" \".join([self.decode(x) for x in tokens])\n",
    "\n",
    "    def fit(self, data, max_words=10000, verbose=True):\n",
    "        word2cnt = defaultdict(lambda: 0)\n",
    "        word_list = self._extract_words(data)\n",
    "        with tqdm(total=len(word_list)) as pbar:\n",
    "            for idx, word in enumerate(word_list):\n",
    "                if len(word) == 0:\n",
    "                    continue\n",
    "                word2cnt[word] += 1\n",
    "                if idx % 10000 == 0:\n",
    "                    pbar.set_description(f'Processed {idx + 1}/{len(word_list)}')\n",
    "                pbar.update(1)\n",
    "        loaded_cnt = len(word2cnt)\n",
    "        if verbose:\n",
    "            print(f'Loaded {loaded_cnt} unique words from {len(word_list)} corpus'\n",
    "                  f'({100. * loaded_cnt / len(word_list)} %)')\n",
    "        words_limit = max_words - len(self.tokens)\n",
    "        popular_words = sorted(\n",
    "            word2cnt.items(), reverse=True, key=lambda x: x[1])[:words_limit]\n",
    "        if verbose:\n",
    "            print(f'20 most popular words:\\n{popular_words[:20]}')\n",
    "\n",
    "        self._word2id = {item.name: item.value for item in self.tokens}\n",
    "        for word, _ in popular_words:\n",
    "            self._word2id[word] = len(self._word2id)\n",
    "        assert len(self._word2id) <= max_words\n",
    "        self._id2word = {v: k for k, v in self._word2id.items()}\n",
    "\n",
    "    def _extract_words(self, data):\n",
    "        words = []\n",
    "        if isinstance(data, list):\n",
    "            for line in data:\n",
    "                assert isinstance(line, str)\n",
    "                words.extend([self.preprocessor(x) for x in line.split(' ')])\n",
    "        else:\n",
    "            assert isinstance(data, str)\n",
    "            words.extend([self.preprocessor(x) for x in data.split(' ')])\n",
    "        return words\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_preprocessor(word):\n",
    "        return re.sub('[^A-Za-zА-Яа-я\\s]+', '', word).lower().strip()\n",
    "\n",
    "    @property\n",
    "    def start_token(self):\n",
    "        return int(self.tokens.START)\n",
    "\n",
    "    @property\n",
    "    def stop_token(self):\n",
    "        return int(self.tokens.STOP)\n",
    "\n",
    "    @property\n",
    "    def pad_token(self):\n",
    "        return int(self.tokens.PAD)\n",
    "\n",
    "    @property\n",
    "    def unk_token(self):\n",
    "        return int(self.tokens.UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "25f92826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "class OpusTranslationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        en_data_path,\n",
    "        ru_data_path,\n",
    "        en_tokenizer,\n",
    "        ru_tokenizer,\n",
    "        fit_tokenizer=False,\n",
    "        max_words=40\n",
    "    ):\n",
    "        self.en_lang = en_tokenizer\n",
    "        self.ru_lang = ru_tokenizer\n",
    "        \n",
    "        with open(en_data_path, 'rt', encoding='utf-8') as f:\n",
    "            self._inputs = f.readlines()\n",
    "        with open(ru_data_path, 'rt', encoding='utf-8') as f:\n",
    "            self._targets = f.readlines()\n",
    "       \n",
    "        self._max_words = max_words\n",
    "        self._filter_samples()\n",
    "        assert len(self._inputs) == len(self._targets)\n",
    "        \n",
    "        if fit_tokenizer:\n",
    "            self.en_lang.fit(self._inputs)\n",
    "            self.ru_lang.fit(self._targets)   \n",
    "\n",
    "    def _filter_samples(self):\n",
    "        valid_indices = []\n",
    "        \n",
    "        def valid(sample):\n",
    "            return 10 < len(sample.split(' ')) <= self._max_words - 2\n",
    "\n",
    "        for i in range(len(self._inputs)):\n",
    "            if valid(self._inputs[i]) and valid(self._targets[i]):\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        self._inputs = [self._inputs[x] for x in valid_indices]\n",
    "        self._targets = [self._targets[x] for x in valid_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._inputs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _pad(seq, tokenizer, size, prepadding=False):\n",
    "        pad_length = size - len(seq)\n",
    "        if pad_length == 0:\n",
    "            return seq\n",
    "        if prepadding:\n",
    "            return pad_length * [tokenizer.pad_token] + seq\n",
    "        else:\n",
    "            return seq + pad_length * [tokenizer.pad_token]\n",
    "\n",
    "    def __getitem__(self, index):       \n",
    "        in_sentence = self._inputs[index]\n",
    "        encoder_input = self.en_lang.encode_line(in_sentence)[:self._max_words]\n",
    "        encoder_input = OpusTranslationDataset._pad(encoder_input, self.en_lang, self._max_words, prepadding=True)\n",
    "\n",
    "        target_sentence = self._targets[index]\n",
    "        decoder_output = self.ru_lang.encode_line(target_sentence)[:self._max_words] + [self.ru_lang.stop_token]\n",
    "        decoder_input = [self.ru_lang.start_token] + decoder_output\n",
    "        decoder_output = OpusTranslationDataset._pad(decoder_output, self.ru_lang, self._max_words)\n",
    "        decoder_input = OpusTranslationDataset._pad(decoder_input, self.ru_lang, self._max_words)\n",
    "        \n",
    "        return {\n",
    "            'encoder_input': np.asarray(encoder_input),\n",
    "            'decoder_input': np.asarray(decoder_input),\n",
    "            'decoder_output': np.asarray(decoder_output),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "576e7ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 4480001/4485356:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 4374378/4485356 [00:03<00:00, 1344229.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 105950 unique words from 4485356 corpus(2.3621313447583647 %)\n",
      "20 most popular words:\n",
      "[('the', 319859), ('of', 175422), ('and', 144455), ('to', 130756), ('in', 98446), ('a', 74961), ('for', 48953), ('that', 48535), ('on', 40347), ('is', 39394), ('you', 38259), ('i', 33471), ('with', 31357), ('be', 28517), ('it', 28059), ('as', 24946), ('by', 23479), ('was', 22478), ('this', 21546), ('are', 19783)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 3950001/3951889:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 3855738/3951889 [00:03<00:00, 1207533.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 231795 unique words from 3951889 corpus(5.8654228395585 %)\n",
      "20 most popular words:\n",
      "[('в', 151362), ('и', 139293), ('на', 56561), ('что', 47700), ('с', 45879), ('не', 42111), ('по', 35579), ('я', 28110), ('для', 25541), ('о', 20299), ('к', 19514), ('как', 18566), ('это', 16668), ('мы', 14845), ('а', 14565), ('из', 14542), ('он', 13587), ('за', 13556), ('от', 13487), ('его', 13121)]\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = 'data'\n",
    "TRAIN_RU = os.path.join(DATA_ROOT, 'train_ru.txt')\n",
    "TRAIN_EN = os.path.join(DATA_ROOT, 'train_en.txt')\n",
    "VAL_RU = os.path.join(DATA_ROOT, 'test_ru.txt')\n",
    "VAL_EN = os.path.join(DATA_ROOT, 'test_en.txt')\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "\n",
    "en_tokenizer = Tokenizer()\n",
    "ru_tokenizer = Tokenizer()\n",
    "\n",
    "train_dataset = OpusTranslationDataset(\n",
    "    en_data_path=TRAIN_EN,\n",
    "    ru_data_path=TRAIN_RU,\n",
    "    en_tokenizer=en_tokenizer,\n",
    "    ru_tokenizer=ru_tokenizer,\n",
    "    fit_tokenizer=True,\n",
    "    max_words=40\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = OpusTranslationDataset(\n",
    "    en_data_path=VAL_EN,\n",
    "    ru_data_path=VAL_RU,\n",
    "    en_tokenizer=en_tokenizer,\n",
    "    ru_tokenizer=ru_tokenizer,\n",
    "    max_words=40\n",
    ")\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2b82c6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD encouraging and facilitating networking among educational and training institutions in developed countries with those in developing countries particularly ldcs to enhance voluntary services in UNK\n",
      "START необходимо поощрять и стимулировать развитие UNK связей между UNK и UNK UNK UNK в развитых и развивающихся странах особенно нрс в интересах расширения UNK участия в программах UNK STOP PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "необходимо поощрять и стимулировать развитие UNK связей между UNK и UNK UNK UNK в развитых и развивающихся странах особенно нрс в интересах расширения UNK участия в программах UNK STOP PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[1200]\n",
    "print(train_dataset.en_lang.decode_line(item['encoder_input']))\n",
    "print(train_dataset.ru_lang.decode_line(item['decoder_input']))\n",
    "print(train_dataset.ru_lang.decode_line(item['decoder_output']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3290139",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764befb9",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "03d86215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ffedce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_embeddings=10000, hidden_size=256, embedding_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_norm=True\n",
    "        )\n",
    "        self._enc_lstm_0 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            dropout=0.,\n",
    "        )\n",
    "        self._enc_lstm_1 = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            dropout=0.,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        embedded = self.embedding(tokens)\n",
    "        enc0_out, (enc0_h, enc0_c) = self._enc_lstm_0(embedded)\n",
    "        enc1_out, (enc1_h, enc1_c) = self._enc_lstm_1(enc0_out)\n",
    "        return {\n",
    "            'enc0_h': enc0_h,\n",
    "            'enc0_c': enc0_c,\n",
    "            'enc1_h': enc1_h,\n",
    "            'enc1_c': enc1_c,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a0dc8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "01a45747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.Tensor(item['encoder_input']).int().unsqueeze(0)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8994f86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc0_h\ttorch.Size([1, 1, 256])\n",
      "enc0_c\ttorch.Size([1, 1, 256])\n",
      "enc1_h\ttorch.Size([1, 1, 256])\n",
      "enc1_c\ttorch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "for k, v in enc(sample).items():\n",
    "    print(f\"{k}\\t{v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9f6fec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_embeddings=10000, hidden_size=256, embedding_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_norm=True\n",
    "        )\n",
    "        self._dec_lstm_0 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            dropout=0.,\n",
    "        )\n",
    "        self._dec_lstm_1 = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            dropout=0.,\n",
    "        )\n",
    "        self._dec_dense = nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=num_embeddings,\n",
    "        )\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, tokens, context):\n",
    "        dec0_in_h = context['enc0_h']\n",
    "        dec0_in_c = context['enc0_c']\n",
    "        dec1_in_h = context['enc1_h']\n",
    "        dec1_in_c = context['enc1_c']\n",
    "        \n",
    "        embedded = self.embedding(tokens)\n",
    "        dec0_out, (dec0_h, dec0_c) = self._dec_lstm_0(embedded, (dec0_in_h, dec0_in_c))\n",
    "        dec1_out, (dec1_h, dec1_c) = self._dec_lstm_1(dec0_out, (dec1_in_h, dec1_in_c))\n",
    "        \n",
    "        logits = self._dec_dense(dec1_out)\n",
    "        scores = self.softmax(logits)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def infer_token(self, token, context):\n",
    "        dec0_in_h = context['enc0_h']\n",
    "        dec0_in_c = context['enc0_c']\n",
    "        dec1_in_h = context['enc1_h']\n",
    "        dec1_in_c = context['enc1_c']\n",
    "        \n",
    "        assert tuple(token.shape) == (1, 1)\n",
    "        \n",
    "        embedding = self.embedding(token)\n",
    "        dec0_out, (dec0_h, dec0_c) = self._dec_lstm_0(embedding, (dec0_in_h, dec0_in_c))\n",
    "        dec1_out, (dec1_h, dec1_c) = self._dec_lstm_1(dec0_out, (dec1_in_h, dec1_in_c))\n",
    "        \n",
    "        logits = self._dec_dense(dec1_out)\n",
    "        scores = self.softmax(logits)\n",
    "        \n",
    "        return scores, {\n",
    "            'enc0_h': dec0_h,\n",
    "            'enc0_c': dec0_c,\n",
    "            'enc1_h': dec1_h,\n",
    "            'enc1_c': dec1_c,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "70cef8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    def __init__(self, max_length=40, start_token=SpecToken.START, stop_token=SpecToken.STOP):\n",
    "        super(Translator, self).__init__()\n",
    "        self._encoder = Encoder()\n",
    "        self._decoder = Decoder()\n",
    "        self._loss = nn.CrossEntropyLoss()\n",
    "        self._start_token = torch.Tensor([int(start_token)]).long()\n",
    "        self._stop_token = torch.Tensor([int(stop_token)]).long()\n",
    "        self._max_length = max_length\n",
    "\n",
    "    def forward(self, tokens, dec_input=None, dec_target=None):\n",
    "        context = self._encoder(tokens)\n",
    "        if self.training:\n",
    "            scores = self._decoder(dec_input, context)\n",
    "            b, n, c = scores.shape\n",
    "            scores_reshaped = torch.reshape(scores, [b * n, -1])\n",
    "            targets_reshaped = torch.reshape(dec_target, [b * n]).long()           \n",
    "            loss = self._loss(\n",
    "                scores_reshaped,\n",
    "                targets_reshaped,\n",
    "            )\n",
    "            return loss\n",
    "        else:\n",
    "            b, n = tokens.shape\n",
    "            assert b == 1, f\"Inference is now available only for batch size 1\"\n",
    "            context = self._encoder(tokens)\n",
    "                       \n",
    "            res = []\n",
    "            cur_token = self._start_token.unsqueeze(0)\n",
    "            scores, context = self._decoder.infer_token(cur_token, context)\n",
    "            res.append(torch.argmax(scores[0, 0]))\n",
    "            \n",
    "            \n",
    "            for token_idx in range(1, self._max_length):\n",
    "                cur_token = res[-1]\n",
    "                if cur_token == self._stop_token:\n",
    "                    break\n",
    "                cur_token = cur_token.unsqueeze(0).unsqueeze(0)\n",
    "                scores, context = self._decoder.infer_token(cur_token, context)\n",
    "                res.append(torch.argmax(scores[0, 0]))\n",
    "                \n",
    "            return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "328dc82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Translator(\n",
       "  (_encoder): Encoder(\n",
       "    (embedding): Embedding(10000, 128, max_norm=True)\n",
       "    (_enc_lstm_0): LSTM(128, 256, batch_first=True)\n",
       "    (_enc_lstm_1): LSTM(256, 256, batch_first=True)\n",
       "  )\n",
       "  (_decoder): Decoder(\n",
       "    (embedding): Embedding(10000, 128, max_norm=True)\n",
       "    (_dec_lstm_0): LSTM(128, 256, batch_first=True)\n",
       "    (_dec_lstm_1): LSTM(256, 256, batch_first=True)\n",
       "    (_dec_dense): Linear(in_features=256, out_features=10000, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       "  (_loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Translator()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "95521d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.2099, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.Tensor(item['encoder_input']).int().unsqueeze(0)\n",
    "sample_dec = torch.Tensor(item['decoder_input']).int().unsqueeze(0)\n",
    "target = torch.Tensor(item['decoder_output']).int().unsqueeze(0)\n",
    "\n",
    "loss = model(sample, sample_dec, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "924b3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "r = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b2f1bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START START\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.ru_lang.decode_line(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b38e6c",
   "metadata": {},
   "source": [
    "## Train loop draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6b5a054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Loss: 9.346098899841309\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 100 Loss: 2.2243309020996094\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 200 Loss: 2.5397326946258545\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 300 Loss: 4.161799907684326\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 400 Loss: 3.258639097213745\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 500 Loss: 2.9066083431243896\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 600 Loss: 2.0057032108306885\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 700 Loss: 3.516080379486084\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 800 Loss: 2.524286985397339\n",
      "Tokens shape: torch.Size([4, 40])\n",
      "Step 900 Loss: 2.571601390838623\n",
      "Tokens shape: torch.Size([4, 40])\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import RMSprop\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_iter = iter(train_dataloader)\n",
    "optimizer = RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    sample = next(train_iter)\n",
    "    in_tokens = torch.Tensor(sample['encoder_input']).long()\n",
    "    dec_inputs = torch.Tensor(sample['decoder_input']).long()\n",
    "    dec_targets = torch.Tensor(sample['decoder_output']).long()\n",
    "    \n",
    "    \n",
    "    loss = model(\n",
    "        tokens=in_tokens,\n",
    "        dec_input=dec_inputs,\n",
    "        dec_target=dec_targets,\n",
    "    )\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f'Step {step} Loss: {loss}')\n",
    "        print(f'Tokens shape: {in_tokens.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "20a7834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(val_dataloader))\n",
    "in_tokens = torch.Tensor(sample['encoder_input']).long()\n",
    "dec_inputs = torch.Tensor(sample['decoder_input']).long()\n",
    "dec_targets = torch.Tensor(sample['decoder_output']).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7219ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD our booking system is secure search a hotel in finland compare rates and make your booking\n",
      "вы UNK подтверждение UNK UNK гостиницы по UNK наша система UNK и UNK выберите отель в UNK UNK UNK цены и UNK ваш заказ STOP PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "sample = in_tokens[0]\n",
    "target = dec_targets[0]\n",
    "print(train_dataset.en_lang.decode_line(sample))\n",
    "print(train_dataset.ru_lang.decode_line(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5203b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prediction = model(torch.Tensor(sample).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4095f573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab0a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "38185987ea3777795b9414fc09eb23f733b2944bc9dca6587ddb380c72fa37b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
